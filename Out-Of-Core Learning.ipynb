{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-Of-Core Learning and Online Learning\n",
    "\n",
    "This note is intended to build a sentiment model but with bigger feature space and the data is live-streaming. The considered example situation is like tweets or live news announcement. The difficulty for these cases is that the word features of the model as well as the vector space always change. Thus our learning process is online and in real-time, called **out-of-core learning**.\n",
    "\n",
    "In the previous cases, we constructed the word feature space from **all** documents to train a logistic model or use Naive Bayes classifier for sentiment analysis. Here, we will use the `SGDClassifier` for minibatches of documents to train a logistic regression model. There are **50000** documents in the entire corpus, divided by **45000** as training set and **5000** as test set. We simulate that in each period, there are **100/1000** incoming documents, so the number of minibatches is **100/1000**. Eventually we have **450/45** iterations of data streamming and online training.\n",
    "\n",
    "The `Hashingvectorizer` in Python helps us to build a hash table to store the word features during all streaming processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/train/pos/ 1\n",
      "aclImdb/train/neg/ 0\n",
      "aclImdb/train/pos/ 1\n",
      "aclImdb/train/neg/ 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for type in ['train', 'test']:\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        path = 'aclImdb/train/%s/' % sentiment\n",
    "        if sentiment == 'pos': y = 1\n",
    "        if sentiment == 'neg': y = 0\n",
    "        print (path, y)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r') as infile:\n",
    "                text = infile.read()\n",
    "                df = df.append([[text, y]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...          1\n",
       "1  Homelessness (or Houselessness as George Carli...          1\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...          1\n",
       "3  This is easily the most underrated film inn th...          1\n",
       "4  This is not the typical Mel Brooks film. It wa...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to randomize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>Often tagged as a comedy, The Man In The White...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>After Chaplin made one of his best films: Doug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  Often tagged as a comedy, The Man In The White...          1\n",
       "19602  After Chaplin made one of his best films: Doug...          0\n",
       "45519  ***SPOILER*** Do not read this, if you think a...          0\n",
       "25747  hi for all the people who have seen this wonde...          1\n",
       "42642  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data **movie_data.csv**, we have **50000** documents, each of them is composed of (review, sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Here there exist a lot '< br/>' in data. Here we use regular expression to move it and remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stop_words = set(stopwords.words('English'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())+ ' '.join(emoticons).replace('-', '')\n",
    "    return [x for x in text.split() if x not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc stream function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    \"\"\"\n",
    "    This function is to help each time, we stream ONE text.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            #print(line[-1])\n",
    "            ## line[-1] is blank\n",
    "            line = line.rstrip()\n",
    "            text, label = line[:-2], int(line[-1])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stream_docs` is to help each time, we stream ONE text. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"I watched like 8 or 9 Herzog movies and none of them had any impact on me.<br /><br />I watched several documentaries about him. He is obviously an intelligent man, with great knowledge about films and passion for making them, but does this makes him a good director. Definitely NO! A complete anti-talent. He can make a good documentary because of previously mentioned traits, but a film with actors \\x96 never!<br /><br />He can\\'t direct nor write. His screenplays are full of badly thought out situations, and many situations/dialogues in his movies are so childishly and badly done that they cannot be hidden behind the word \"\"art\"\" in any sense. No way. Not to mention the unskillful direction, so amateurish-like. To say that he wants to direct like that and write crap like that is a lie.<br /><br />Like the scene when Scheitz gets arrested and Storszek hides in the back of the store. WHO IS HE KIDDING?<br /><br />He is a cheater; he knows what fake intellectuals and critics want. He knows what elements he needs to put in the script to get your their attention and empty praising. Never mind the rest of the script and sloppy direction.<br /><br />Just look at Julio Medem. If Herzog can make a movie like Medem can, then I might re-check his old movies and try to find talent in them.\"',\n",
       " 1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs('movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Function\n",
    "\n",
    "This function helps to stream the doc with **batch_size** times, until we went through all of them. In the following, we use **batch_size=100/1000**, mimic in a certian period, we have **100/1000** incoming documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, batch_size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for batch in range(batch_size):\n",
    "            text, label = next(doc_stream)\n",
    "            #print (text, label)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer)\n",
    "sgd = SGDClassifier(loss='log', random_state=1, n_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 1000 minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training text data\n",
    "\"\"\"\n",
    "classes = np.array([0, 1])\n",
    "for iteration in range(45):\n",
    "    X, y = get_minibatch(doc_stream, batch_size = 1000)\n",
    "    if not X: break\n",
    "    X = vect.transform(X)\n",
    "    sgd.partial_fit(X, y, classes = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.876\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, batch_size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % sgd.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model with 100 minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = np.array([0, 1])\n",
    "for iteration in range(450):\n",
    "    X, y = get_minibatch(doc_stream, batch_size = 100)\n",
    "    if not X: break\n",
    "    X = vect.transform(X)\n",
    "    sgd.partial_fit(X, y, classes = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.877\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, batch_size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % sgd.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Minibatch Learning and Batch Learning\n",
    "\n",
    "Previous we considered either 100 or 1000 minibatches to perform the logistic regression for sentiment model. How does it compare with the entire batches? Here we revisit the same data, but use Nive Bayes classifier from sklearn and NLTK.\n",
    "\n",
    "### Construct the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hd = open('movie_data.csv', 'r')\n",
    "next(hd)\n",
    "corpus_words = []\n",
    "for review in hd:\n",
    "    review = review.rstrip()\n",
    "    #words = [ps.stem(x.lower()) for x in nltk.word_tokenize(str(review[0]))\n",
    "    #            if x not in stop_words if x not in punctuations if x != \"''\"]\n",
    "    words = tokenizer(review[:-2])\n",
    "    for word in words:\n",
    "        corpus_words.append(word)\n",
    "\n",
    "corpus_words = nltk.FreqDist(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75803 [('movie', 88056), ('film', 80288), ('one', 53576), ('like', 40548), ('good', 30280), ('time', 25446), ('even', 25290), ('would', 24872), ('story', 23962), ('really', 23470)]\n"
     ]
    }
   ],
   "source": [
    "print (len(corpus_words), corpus_words.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use 5000 most common words for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('claus', 160)\n"
     ]
    }
   ],
   "source": [
    "print (corpus_words.most_common(5000)[4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_features = [x for x, y in corpus_words.most_common(5000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the feature vector space\n",
    "\n",
    "In the following, we go through all documents and examine each tokenized word. If they exist in **corpus_words**, we label 'Ture'; otherwise 'False'. In other words, here we simply consider the **bag of words** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(review):\n",
    "    features = {}\n",
    "    review_words = tokenizer(review)\n",
    "    for word in word_features:\n",
    "        features[word] = (word in review_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hd = open('movie_data.csv', 'r')\n",
    "next(hd)\n",
    "allData = []\n",
    "for review in hd:\n",
    "    review = review.rstrip()\n",
    "    category = review[-1]\n",
    "    #print (review[:-2])\n",
    "    allData.append((get_features(review[:-2]), category))\n",
    "    #allData.append(review[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(allData, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8612\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "print ('Accuracy:', nltk.classify.accuracy(classifier, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Accuracy: 0.8620666666666666\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(train)\n",
    "print ('MNB Accuracy:', nltk.classify.accuracy(MNB_classifier, test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
