{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python NLP Library Implmention\n",
    "\n",
    "### Content:\n",
    "  *  1 NLTK\n",
    "  *  2 Word Feature Vector Space\n",
    "  *  3 TF-IDF representation\n",
    "  *  4 Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence1 = \"At eight on Thursday morning Arthur felt very good good, but not perfect.\"\n",
    "sentence2 = \"Thursday night is good!\"\n",
    "sentence3 = \"Although Arthur is is feeling good at eight feel\"\n",
    "article = \"At eight on Thursday morning Arthur felt very good good, but not prefect. Thursday night is good! Although Arthur is is feeling good at eight feel.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['At eight on Thursday morning Arthur felt very good good, but not prefect.', 'Thursday night is good!', 'Although Arthur is is feeling good at eight feel.']\n"
     ]
    }
   ],
   "source": [
    "print (type(nltk.sent_tokenize(article)))\n",
    "print (nltk.sent_tokenize(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Normalization\n",
    "\n",
    "Unify all words in lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at eight on thursday morning arthur felt very good good, but not perfect.\n"
     ]
    }
   ],
   "source": [
    "sentenc1 = sentence1.lower()\n",
    "print (sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at eight on thursday morning arthur felt very good good, but not perfect.\n",
      "<class 'list'>\n",
      "['at', 'eight', 'on', 'thursday', 'morning', 'arthur', 'felt', 'very', 'good', 'good', ',', 'but', 'not', 'perfect', '.']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sentence1)\n",
    "print (sentence1)\n",
    "print (type(words))\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLTK, the word tokenizer can also be implemented using reg expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'eight', 'on', 'thursday', 'morning', 'arthur', 'felt', 'very', 'good', 'good', 'but', 'not', 'perfect']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words2 = tokenizer.tokenize(sentence1)\n",
    "print (words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that compared with the `nltk.word_tokenize`, `RegespTokenizer` has advantage which removes punctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{')', '%', '|', '?', '-', '^', '>', '~', '*', '[', '#', '$', ']', '=', '.', \"'\", '{', '&', '_', ',', '/', '}', '`', '(', '\\\\', '+', ';', '\"', '<', '!', '@', ':'}\n"
     ]
    }
   ],
   "source": [
    "print (punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'eight', 'on', 'thursday', 'morning', 'arthur', 'felt', 'very', 'good', 'good', 'but', 'not', 'perfect']\n"
     ]
    }
   ],
   "source": [
    "print ([x for x in words if x not in punctuations])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('English'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'isn', 't', 've', 'he', 'with', 'will', 'yourselves', 'about', 'll', 'o', 'did', 'because', 'it', 'of', 'they', 'what', 'those', 'be', 'hasn', 'while', 'and', 'down', 'these', 'which', 'myself', 'at', 'were', 'does', 'where', 'haven', 'shouldn', 'weren', 'has', 'an', 'on', 'doing', 'as', 'was', 'only', 'won', 'through', 'themselves', 'but', 'before', 'if', 'here', 'y', 'its', 'them', 'out', 'to', 'now', 'most', 'hadn', 're', 'under', 'how', 'yourself', 'not', 'between', 'by', 'mustn', 'own', 'such', 'our', 'there', 'off', 'some', 'until', 'then', 'that', 'needn', 'in', 'itself', 's', 'i', 'after', 'over', 'is', 'me', 'don', 'just', 'whom', 'during', 'having', 'aren', 'very', 'should', 'you', 'have', 'am', 'this', 'his', 'wouldn', 'can', 'no', 'again', 'all', 'up', 'ain', 'ma', 'too', 'didn', 'any', 'my', 'ourselves', 'from', 'so', 'm', 'few', 'a', 'above', 'into', 'when', 'couldn', 'yours', 'who', 'further', 'once', 'the', 'why', 'theirs', 'we', 'do', 'against', 'mightn', 'doesn', 'hers', 'him', 'her', 'herself', 'd', 'other', 'below', 'she', 'shan', 'same', 'more', 'had', 'for', 'each', 'or', 'are', 'your', 'nor', 'both', 'their', 'being', 'wasn', 'ours', 'been', 'than'}\n"
     ]
    }
   ],
   "source": [
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eight', 'thursday', 'morning', 'arthur', 'felt', 'good', 'good', ',', 'perfect', '.']\n"
     ]
    }
   ],
   "source": [
    "print ([x for x in words if x not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Stemming\n",
    "\n",
    "Stemmers are used to turn all verbs' tenses to the present tense and plural nouns to singular noun. Usually tenses do not bring additional information to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.1 Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'eight', 'on', 'thursday', 'morn', 'arthur', 'felt', 'veri', 'good', 'good', ',', 'but', 'not', 'perfect', '.']\n"
     ]
    }
   ],
   "source": [
    "print ([ps.stem(x) for x in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here the porter stemmer is **unable** to replace 'felt' with stemmed 'feel', so the porter stemmer doesn't work perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('run', 'ran', 'feel', 'felt')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('running'), ps.stem('ran'), ps.stem('feeling'), ps.stem('felt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2 Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('run', 'ran', 'feel', 'felt')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('running'), ss.stem('ran'), ss.stem('feeling'), ss.stem('felt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eight', 'good', 'arthur', 'thursday', 'felt', 'perfect', 'morn'}\n"
     ]
    }
   ],
   "source": [
    "print (set([ps.stem(x.lower()) for x in words if x not in punctuations if x not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the word space is smaller than directly word tokenizing the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Feature Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At eight on Thursday morning Arthur felt very good good, but not prefect. Thursday night is good! Although Arthur is is feeling good at eight feel.\n"
     ]
    }
   ],
   "source": [
    "print (article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 Feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at eight on thursday morning arthur felt very good good, but not prefect.\n",
      "['eight', 'thursday', 'morn', 'arthur', 'felt', 'good', 'good', 'prefect']\n",
      "thursday night is good!\n",
      "['thursday', 'night', 'good']\n",
      "although arthur is is feeling good at eight feel.\n",
      "['although', 'arthur', 'feel', 'good', 'eight', 'feel']\n"
     ]
    }
   ],
   "source": [
    "all_corpus_words = []\n",
    "for sentence in nltk.sent_tokenize(article.lower()):\n",
    "    print (sentence)\n",
    "    words = [ps.stem(x) for x in nltk.word_tokenize(sentence) \n",
    "                 if x not in punctuations if x not in stop_words]\n",
    "    all_corpus_words += words\n",
    "    print (words)\n",
    "    \n",
    "all_corpus_words = set(all_corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prefect', 'night', 'eight', 'feel', 'good', 'arthur', 'thursday', 'felt', 'although', 'morn'}\n"
     ]
    }
   ],
   "source": [
    "print (all_corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 words in the word sets **`all_corpus_words`** in the **article**, so our vectore space is 10-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(review):\n",
    "    features = {}\n",
    "    review_words = set([x.lower() for x in nltk.word_tokenize(str(review)) \n",
    "                     if x not in stop_words if x not in punctuations])\n",
    "    for word in all_corpus_words:\n",
    "        features[word] = (word in review_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Basic representaiton (0/1) for feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'prefect': True, 'night': False, 'feel': False, 'felt': True, 'thursday': True, 'eight': True, 'arthur': True, 'although': False, 'good': True, 'morn': False}\n",
      "2 {'prefect': False, 'night': True, 'feel': False, 'felt': False, 'thursday': True, 'eight': False, 'arthur': False, 'although': False, 'good': True, 'morn': False}\n",
      "3 {'prefect': False, 'night': False, 'feel': True, 'felt': False, 'thursday': False, 'eight': True, 'arthur': True, 'although': True, 'good': True, 'morn': False}\n"
     ]
    }
   ],
   "source": [
    "sent =1\n",
    "for sentence in nltk.sent_tokenize(article):\n",
    "    print (sent, get_features(sentence))\n",
    "    sent +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sklearn: TF-IDF representation\n",
    "\n",
    "Here we are going to use sklean to compute TF-IDF for each word feature, rather than simply counting `exist` or `nonexist`. In sklearn, the tf-idf(w,d) of word **w** in document **d** is \n",
    "\n",
    "### $$\\textrm{tf-idf(w,d)} = \\textrm{tf (w,d)}\\times (1+ \\textrm{idf(w,d)})= \\textrm{tf (w,d)}\\times (1+\\log{\\Big( \\frac{n_d+1}{\\textrm{df(w)}+1} \\Big) })$$\n",
    "\n",
    "**tf(w,d)**: term-frequency of word **w** in document **d**. **df(w)** is the document-frequency, number of documents where the word **w** appeared. **$n_d$** is the total number of documents in all corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 TF representation\n",
    "\n",
    "TF means `term-frequency`, the frequency of the word appeared in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "doc = np.array([sentence1, sentence2, sentence3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eight': 6, 'at': 4, 'arthur': 3, 'thursday': 17, 'very': 18, 'felt': 9, 'feel': 7, 'on': 15, 'Thursday': 2, 'night': 13, 'not': 14, 'feeling': 8, 'good': 10, 'morning': 12, 'Arthur': 1, 'but': 5, 'perfect': 16, 'is': 11, 'Although': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(lowercase=False)\n",
    "bag = count.fit_transform(doc)\n",
    "print (count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 10)\t2\n",
      "  (0, 18)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 10)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 11)\t2\n",
      "  (2, 10)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 4)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bag` is a matrix, `(i,j)` telling us the TF of the `j-th` word in the `i-th` sentence. For example, `(0,10)=2` means that the word **good** appeared twice in **sentence1**, and `(1,10)=1` shows it appeared once in **sentence2**.\n",
    "\n",
    "We see that `CountVectorizer` only does word tokenizer, without removing `stopwords` such as **very**, **not**, **but**, **on**, **at**, **is**, distinguishing the Captial and little **thursday**/**Thursday**, **Arthur**/**arthur**, and the verb tense **feeling**/**feel**. After these repeats, the dimensionality is still 10. We can turn on the `lowercase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eight': 4, 'at': 2, 'arthur': 1, 'thursday': 15, 'very': 16, 'felt': 7, 'feel': 5, 'on': 13, 'night': 11, 'not': 12, 'feeling': 6, 'good': 8, 'morning': 10, 'but': 3, 'perfect': 14, 'is': 9, 'although': 0}\n"
     ]
    }
   ],
   "source": [
    "'''The deault to use CountVectorizer is lowercase = True'''\n",
    "count = CountVectorizer()\n",
    "bag = count.fit_transform(doc)\n",
    "print (count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the difference between **thursday**/**Thursday**, **Arthur**/**arthur** is eliminated. This can help us reduce the vector space dimensionality by two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature index\n",
    "\n",
    "In `CountVectorizer`, we can implement `stop_words`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'night': 6, 'feel': 1, 'feeling': 2, 'arthur': 0, 'good': 4, 'felt': 3, 'morning': 5, 'thursday': 8, 'perfect': 7}\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(stop_words='english')\n",
    "bag = count.fit_transform(doc)\n",
    "print (count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare `bag` with the `all_corpus_words` from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prefect', 'night', 'eight', 'feel', 'good', 'arthur', 'thursday', 'felt', 'although', 'morn'}\n"
     ]
    }
   ],
   "source": [
    "print (all_corpus_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here the sklearn's `CountVector` with `stop_words` also removed words **although** and **eight**, but we didn't do stemmer so there is still **feeling**. Now `CountVector` converts the article to a 9-dimensional vector space. On the other hand, previously we implemented stemmer to `all_corpus_word` so there is no **feeling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'at eight on thursday morning arthur felt very good good, but not perfect.'\n",
      " 'Thursday night is good!'\n",
      " 'Although Arthur is is feeling good at eight feel']\n"
     ]
    }
   ],
   "source": [
    "print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 2 1 0 1 1]\n",
      " [0 0 0 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print (bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **sentence2**, we have 'Thursday night is good!'. In the vector space, we have 'thursday': 8, 'night': 6, 'good': 4, so `[0 0 0 0 1 0 1 0 1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 n-gram representation (n>1)\n",
    "\n",
    "The above methid is called uni-gram, i.e. tokenize **each** word. In reality, some words are relevant to each other and usually people use the relevant words **together**, like 'bus stop'. In the following, we do two-gram representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twograms = CountVectorizer(ngram_range=(1,2), stop_words ='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arthur felt': 2, 'feel': 3, 'thursday night': 19, 'good perfect': 11, 'feeling good': 5, 'perfect': 16, 'arthur feeling': 1, 'good good': 10, 'felt good': 7, 'morning': 12, 'night good': 15, 'thursday morning': 18, 'night': 14, 'good feel': 9, 'feeling': 4, 'good': 8, 'arthur': 0, 'thursday': 17, 'felt': 6, 'morning arthur': 13}\n"
     ]
    }
   ],
   "source": [
    "bag = twograms.fit_transform(doc)\n",
    "print (twograms.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 1 1 2 0 1 1 1 1 0 0 1 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1]\n",
      " [1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print (bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 TF-IDF representation\n",
    "\n",
    "Next we need to consider IDF, inverse document frequency. The TF is considered on each **individual** document, or sentence. But we have to consider how often this word appeaed in all corpus. If it is too often, meaning less importance. There are two ways: (1) `tdidfTransformer()`+`CounVectorizer()` and (2) `TdidfVectorizer()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 TfidfTransformer( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32  0.    0.    0.42  0.5   0.42  0.    0.42  0.32]\n",
      " [ 0.    0.    0.    0.    0.43  0.    0.72  0.    0.55]\n",
      " [ 0.44  0.58  0.58  0.    0.35  0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "tfidf = TfidfTransformer()  ## this bag is normalized and removes stop words.\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(bag).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 Using TfidfVectorizer( ) = CountVectorizer( ) + TfidfTransformer( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32  0.    0.    0.42  0.5   0.42  0.    0.42  0.32]\n",
      " [ 0.    0.    0.    0.    0.43  0.    0.72  0.    0.55]\n",
      " [ 0.44  0.58  0.58  0.    0.35  0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None,lowercase=True,preprocessor=None, stop_words ='english')\n",
    "print(tfidf.fit_transform(doc).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'night': 6, 'feel': 1, 'feeling': 2, 'arthur': 0, 'good': 4, 'felt': 3, 'morning': 5, 'thursday': 8, 'perfect': 7}\n",
      "[[1 0 0 1 2 1 0 1 1]\n",
      " [0 0 0 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print (count.vocabulary_)\n",
    "print (bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even the same word, say **good**, has different TF-IDF in each document. In **sentence1**, **sentence2** and **sentence3**, they are 0.5, 0.43, 0.35. It is becuase the `TfidfVectorizer` and `TfidfTransformer` eventually do L2-norm (normalization) in each document. \n",
    "\n",
    "As an example, let us go over all procedures \n",
    "* (1) in **sentence(1,2,3)**, we have `[1,0,0,1,2,1,0,1,1]`, `[0,0,0,0,1,0,1,0,1]` and `[1,1,1,0,1,0,0,0,0]`. So **tf('good',d)=2, 1, 1**, for **d=1,2,3** respectively.\n",
    "* (2) **'good'** appeared in all three documents, so **df('good')=3** and **idf('good')=log(3+1)/(3+1)=0**.\n",
    "* (3) Let's focus on **sentence2**:\n",
    "   *    For \"good\", **tfidf(\"good\",2)=tf('good',2)*(tdf('good')+1)=1*1=1**.\n",
    "   *    For \"Thurday\" and \"night\", **df(\"thursday\")=2** and **df(\"night\")=1**, such that **idf(\"thursdau\")=log(3+1)/(2+1)=0.287** and **idf('night')=log(3+1)/(1+1)=0.693**. \n",
    "   *    Then **tfidf(\"thursday\",2)=1*1.287=1.287** and **tfidf(\"night\",2)=1*1.693=1.693**. Now we have `[0,0,0,0,1,0,1.693,0,1.287]`.\n",
    "* (4) L2-normalize the vector. The normalized constant is 2.35, so `[0,0,0,0,1,0,1.693,0,1.287]` eventually becomes `[0,0,0,0,1/2.35,0,1.693/2.35,0,1.287/2.35]=[0,0,0,0,0.425,0,0.72,0.547]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'at eight on thursday morning arthur felt very good good, but not perfect.',\n",
       "       'Thursday night is good!',\n",
       "       'Although Arthur is is feeling good at eight feel'], \n",
       "      dtype='<U73')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 4, 7, 6]\n",
      "[1 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "index = [count.vocabulary_['thursday'], count.vocabulary_['good'], count.vocabulary_['perfect'],count.vocabulary_['night']]\n",
    "print (index)\n",
    "print (bag.toarray()[0][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 Stemmed `tfidfVectorize( )`\n",
    "\n",
    "Next we can add removing stopwords and stemmer in `tdifvectorize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.stem(t) for t in nltk.word_tokenize(doc) if t not in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'at eight on thursday morning arthur felt very good good, but not perfect.',\n",
       "       'Thursday night is good!',\n",
       "       'Although Arthur is is feeling good at eight feel'], \n",
       "      dtype='<U73')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3   0.    0.39  0.46  0.39  0.    0.39  0.3   0.39]\n",
      " [ 0.    0.    0.    0.43  0.    0.72  0.    0.55  0.  ]\n",
      " [ 0.34  0.9   0.    0.27  0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "stemmedCount = CountVectorizer(stop_words ='english', tokenizer=LemmaTokenizer())\n",
    "stemmedBag = stemmedCount.fit_transform(doc)\n",
    "stemmedTfidf = TfidfTransformer()\n",
    "print (stemmedTfidf.fit_transform(stemmedBag).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'night': 5, 'feel': 1, 'veri': 8, 'good': 3, 'felt': 2, 'arthur': 0, 'thursday': 7, 'perfect': 6, 'morn': 4}\n",
      "[[1 0 1 2 1 0 1 1 1]\n",
      " [0 0 0 1 0 1 0 1 0]\n",
      " [1 2 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print (stemmedCount.vocabulary_)\n",
    "print (stemmedBag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmedTfidf = TfidfVectorizer(strip_accents=None,lowercase=True,preprocessor=None, stop_words ='english', tokenizer=LemmaTokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3   0.    0.39  0.46  0.39  0.    0.39  0.3   0.39]\n",
      " [ 0.    0.    0.    0.43  0.    0.72  0.    0.55  0.  ]\n",
      " [ 0.34  0.9   0.    0.27  0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(stemmedTfidf.fit_transform(doc).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'night': 6, 'feel': 1, 'feeling': 2, 'arthur': 0, 'good': 4, 'felt': 3, 'morning': 5, 'thursday': 8, 'perfect': 7}\n",
      "[[1 0 0 1 2 1 0 1 1]\n",
      " [0 0 0 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print (count.vocabulary_)\n",
    "print (bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 The entire corpus in terms of the tfidf reprentation\n",
    "\n",
    "We need to input the **sentence** in list or np.array format for tfidf.fit_transform(input), not all articles. So before input, we use nltk.sent_tokenize( ) to convert the whole article to sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None,lowercase=True,preprocessor=None, stop_words ='english')\n",
    "doc_tfidf = tfidf.fit_transform(np.array([x for x in nltk.sent_tokenize(article)]))\n",
    "doc_tfidf = tfidf.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32  0.    0.    0.42  0.5   0.42  0.    0.42  0.32]\n",
      " [ 0.    0.    0.    0.    0.43  0.    0.72  0.    0.55]\n",
      " [ 0.44  0.58  0.58  0.    0.35  0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(doc_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.32  0.5   0.42  0.  ]\n"
     ]
    }
   ],
   "source": [
    "print (doc_tfidf.toarray()[0][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4 Latent Dirichlet Allocation (LDA) in Sklearn\n",
    "\n",
    "Next we try to do [LDA model](http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-topics-extraction-with-nmf-lda-py) using sklearn. Latent Dirichlet allocation (LDA) is a topic model that generates topics based on word frequency from a set of documents. LDA is particularly useful for finding reasonably accurate mixtures of topics within a given document set. Let's give another example texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "punctuations = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('English'))\n",
    "ps = PorterStemmer()\n",
    "texts = [ps.stem(x.lower()) for x in nltk.word_tokenize(doc_a) if x not in punctuations if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['brocolli',\n",
       "  'good',\n",
       "  'eat',\n",
       "  'my',\n",
       "  'brother',\n",
       "  'like',\n",
       "  'eat',\n",
       "  'good',\n",
       "  'brocolli',\n",
       "  'mother'],\n",
       " list)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\n",
      "  (0, 5)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 0)\t2\n",
      "  (1, 2)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 2)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 4)\t2\n",
      "  (4, 3)\t1\n",
      "  (4, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000,stop_words='english')\n",
    "features_tf = tf_vectorizer.fit_transform(doc_set)\n",
    "print (features_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "driving pressure mother brother health brocolli good\n",
      "Topic #1:\n",
      "health good brocolli pressure brother driving mother\n",
      "Topic #2:\n",
      "brocolli good brother mother driving pressure health\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 3\n",
    "n_top_words = 20\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(features_tf)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "mother driving brother pressure brocolli health good\n",
      "Topic #1:\n",
      "good brocolli health brother pressure driving mother\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_topics = 2\n",
    "n_top_words = 20\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(features_tf)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "mother brother pressure driving health good brocolli\n",
      "Topic #1:\n",
      "health good brocolli pressure mother driving brother\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "n_features = 1000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(doc_set)\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
